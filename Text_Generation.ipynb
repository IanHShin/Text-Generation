{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xONbny9AajrI"
      },
      "source": [
        "# Markov Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSVMIzYcqE1e",
        "outputId": "a76537e9-09bc-4062-cc47-452a771a52f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Algd_Kyp75Yy",
        "outputId": "1bf2b3d7-c77d-4f30-8dff-76a3d8def8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "\n",
        "pairs = []\n",
        "\n",
        "sample = open('AM.txt', encoding='utf8').read()\n",
        "\n",
        "toke = (word_tokenize(sample))\n",
        "r_toke = [word for word in toke if word.isalnum()]\n",
        "\n",
        "for i in range(len(r_toke)-1):\n",
        "  pair = r_toke[i],r_toke[i+1]\n",
        "  pairs.append(pair)\n",
        "\n",
        "Dict= {}\n",
        "\n",
        "for x in r_toke:\n",
        "  Dict[x] = r_toke.count(x)/len(r_toke)\n",
        "\n",
        "word1 = np.random.choice(list(Dict.keys()), 1, replace = False, p = list(Dict.values())) #this intializes the first word to be the word with the highest prob\n",
        "\n",
        "markov_chain = [word1]\n",
        "\n",
        "word2 = markov_chain.append(np.random.choice(list(Dict.keys()), 40, replace = False, p = list(Dict.values())))\n",
        "print(markov_chain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array(['that'], dtype='<U12'), array(['has', 'that', 'some', 'just', 'thinks', 'Is', 'as', 'meeting',\n",
            "       'to', 'like', 'na', 'they', 'we', 'the', 'right', 'ca', 'go',\n",
            "       'why', 'stay', 'week', 'saying', 'you', 'sort', 'help', 'mine',\n",
            "       'tonight', 'I', 'beside', 'thing', 'she', 'Do', 'Crawling',\n",
            "       'trying', 'mainly', 'are', 'Well', 'him', 'hear', 'back',\n",
            "       'distant'], dtype='<U12')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F_3DO_TaqZ-"
      },
      "source": [
        "# Neural Network (Using TF 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhzlMMHOr8fV"
      },
      "source": [
        "**Bread&Butter**\n",
        "\n",
        "\n",
        "*   Making tensors with *tf.constant* & *tf.Variable*\n",
        "*   Concatenation of two tensors with *tf.concat*\n",
        "*   Reshape data with *tf.reshape*\n",
        "*   Casting tensors to other dtypes with *tf.cast*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_7UNqA8ayoa"
      },
      "source": [
        "#This is a constant therefore it can not change\n",
        "a = tf.constant([[3,2],\n",
        "                 [5,2]])\n",
        "#This is a variable,which can change\n",
        "VA = tf.Variable([[3,2],\n",
        "                 [5,2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vom5AJ7M6kd"
      },
      "source": [
        "**tf.keras.models.Sequential**: Groups a linear stack of layers \n",
        "\n",
        "**tf.keras.layers.Flatten**:Flattens the input. \n",
        "\n",
        "**tf.keras.layers.Dense** A regular Dense NN. (Dense NN\" A linear operation in which every input is connected to every output by a weight)\n",
        "\n",
        "**tf.keras.layers.Dropout**: Applies dropout to the input. (Dropout is propritary regularization technique. It reduces overfitting in NN. It is efficent way of performing model averaging.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_90WETflOCN",
        "outputId": "754faef8-a9ad-44ee-b686-1f5a35b96591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#loading data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "x_train,x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88kcQLuI-YD7"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                   tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                   tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                   tf.keras.layers.Dropout(.2),\n",
        "                                   tf.keras.layers.Dense(10)\n",
        "                                    ])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey9lvstAO9Lz"
      },
      "source": [
        "# Outputs\n",
        "Predictions: Returns a vector of \"logits\" for each class\n",
        "\n",
        "Convert: Converts the above \"logits\" into probabilities for each class\n",
        "\n",
        "loss_fn: Takes a vector of logits, and True index to return a scalar loss for each example. The scalar loss is equal to the negative log probability of the true class. (A zero will be outputted if the model is sure of the correct class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVaIsTadD4pd",
        "outputId": "e822136f-ef10-45b5-b22b-b850ec3982f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "predictions= model(x_train[:1]).numpy()\n",
        "print(\"Logit for each class\\n\", predictions)\n",
        "print()\n",
        "\n",
        "convert = tf.nn.softmax(predictions).numpy()\n",
        "print(\"Probability of logit for each class\\n\",convert)\n",
        "print()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "loss = loss_fn(y_train[:1], predictions).numpy()\n",
        "\n",
        "print(\"Loss function\\n\",loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logit for each class\n",
            " [[-0.08888137  0.05689763  0.38586718 -0.2551431  -0.5943307   0.02116776\n",
            "  -0.26295292  0.0620161   0.51123494 -0.06307518]]\n",
            "\n",
            "Probability of logit for each class\n",
            " [[0.0894252  0.10345964 0.14376085 0.07572746 0.05394436 0.0998283\n",
            "  0.07513835 0.10399055 0.1629623  0.09176296]]\n",
            "\n",
            "Loss function\n",
            " 2.3043036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToE8CgkxTGCe",
        "outputId": "b70da375-a1ef-42c8-986f-45a66e6229be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "model.compile(optimizer = \"adam\",\n",
        "                loss = loss_fn,\n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "model.fit(x_train,y_train,epochs=5) #Adjusts the model parameters to minimize the loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2988 - accuracy: 0.9125\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1438 - accuracy: 0.9569\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1082 - accuracy: 0.9673\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0884 - accuracy: 0.9724\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0762 - accuracy: 0.9761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6d0ee5b1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmJJ1jptXXpL"
      },
      "source": [
        "# Keras implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlq7sk9dSAM",
        "outputId": "e6a135d2-e9ab-4e14-f5e7-d81185e7555f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        " # first neural network with keras tutorial\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbI8nRBSXjgJ",
        "outputId": "50ba68f4-0777-4cc4-95b2-58193564fda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "csv = pd.read_csv(\"/content/pima-indians-diabetes.csv\")\n",
        "\n",
        "x = csv.iloc[:, 0:8]\n",
        "y = csv.iloc[:,8]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12,input_dim = 8, activation = \"relu\"))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation = \"sigmoid\"))\n",
        "\n",
        "model.compile(loss= 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "#model.fit(x,y, epochs = 150, batch_size = 15)\n",
        "\n",
        "loss,accurate = model.evaluate(x,y)\n",
        "print(\"Loss: {}, Accurate: {}\".format(loss,accurate*100))\n",
        "i+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "767/767 [==============================] - 0s 41us/step\n",
            "Loss: 5.202308955062011, Accurate: 65.0586724281311\n",
            "767/767 [==============================] - 0s 31us/step\n",
            "Loss: 5.202308955062011, Accurate: 65.0586724281311\n",
            "767/767 [==============================] - 0s 28us/step\n",
            "Loss: 5.202308955062011, Accurate: 65.0586724281311\n",
            "767/767 [==============================] - 0s 20us/step\n",
            "Loss: 5.202308955062011, Accurate: 65.0586724281311\n",
            "767/767 [==============================] - 0s 24us/step\n",
            "Loss: 5.202308955062011, Accurate: 65.0586724281311\n",
            "767/767 [==============================] - 0s 19us/step\n",
            "Loss: 5.202308955062011, Accurate: 65.0586724281311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37oCkgV1qAC"
      },
      "source": [
        "# Text Generation with Keras\n",
        "(Pre-Process Text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpCaWc6zYzuK",
        "outputId": "09b22b3e-5b33-40d3-d4e4-307e7fae7d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85NrbTfCs8N7",
        "outputId": "a0aae1c2-8543-4a65-be31-aff8c8aca3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError(\"Error\")\n",
        "print(\"GPU a:{}\".format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU a:/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7h5n39c9anz"
      },
      "source": [
        "text = open(\"/content/AM.txt\").read()\n",
        "with tf.device(\"/device:GPU:0\"):\n",
        "  def tokenize(input):\n",
        "    input = input.lower()\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "  processed_inputs = tokenize(text)\n",
        "\n",
        "  chars = sorted(list(set(processed_inputs)))\n",
        "  char_to_num = dict((c,i) for i, c in enumerate(chars))\n",
        "\n",
        "input_len=  len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        " \n",
        "\n",
        "seq_length = 50\n",
        "x_data = []\n",
        "y_data = []\n",
        "print(processed_inputs)\n",
        "for i in range(0, input_len-seq_length):\n",
        "  in_seq = processed_inputs[i:i+seq_length]\n",
        "  print(in_seq)\n",
        "  out_seq = processed_inputs[i+seq_length]\n",
        "  \n",
        "  x_data.append([char_to_num[char] for char in in_seq])\n",
        "  y_data.append(char_to_num[out_seq])\n",
        "\n",
        "n_patterns = len(x_data)\n",
        "X = np.reshape(x_data,(n_patterns,seq_length,1))\n",
        "X = X/float(vocab_len)\n",
        "y= np_utils.to_categorical(y_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1QYf7ruq-qo"
      },
      "source": [
        " \n",
        "\n",
        "\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(256, return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(128))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(y.shape[1], activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  filepath = \"model_weights_saved.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "  desired_callbacks = [checkpoint]\n",
        "\n",
        "  model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)\n",
        "  filename = \"model_weights_saved.hdf5\"\n",
        "  model.load_weights(filename)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "  start = np.random.randint(0, len(x_data) - 1)\n",
        "  pattern = x_data[start]\n",
        "  print(\"Random Seed:\")\n",
        "  print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVSHjx9zlQkd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}